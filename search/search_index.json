{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BlackBIRDS documentation","text":"<p>BlackBIRDS stands for Black-box Bayesian Inference foR Differentiable Simulators. As the name suggests, this is a package to facilitate Bayesian inference on simulators using techniques that require some kind of gradient estimation.</p>"},{"location":"#installation","title":"Installation","text":"<p>The easiest way to install the package is to obtain it from the PyPI repository</p> <pre><code>pip install blackbirds\n</code></pre> <p>Alternatively, you can obtain the latest version directly from git, </p> <pre><code>pip install git+https://github.com/arnauqb/blackbirds\n</code></pre>"},{"location":"api/infer/mcmc/","title":"MCMC","text":""},{"location":"api/infer/mcmc/#blackbirds.infer.mcmc.MALA","title":"<code>blackbirds.infer.mcmc.MALA</code>","text":"<p>         Bases: <code>MCMCKernel</code></p> <p>Class that generates a step in the chain of a Metropolis-Adjusted Langevin Algorithm run.</p> <p>Arguments</p> <ul> <li><code>prior</code>: The prior distribution. Must be differentiable in its argument.</li> <li><code>w</code>: The weight hyperparameter in generalised posterior.</li> <li><code>gradient_clipping_norm</code>: The norm to which the gradients are clipped.</li> <li><code>loss</code>: The loss function used in the exponent of the generalised likelihood term. Maps from data and chain state to loss.</li> <li><code>diff_mode</code>: The differentiation mode to use. Can be either 'reverse' or 'forward'.</li> <li><code>jacobian_chunk_size</code>: The number of rows computed at a time for the model Jacobian. Set to None to compute the full Jacobian at once.</li> <li><code>gradient_horizon</code>: The number of timesteps to use for the gradient horizon. Set 0 to use the full trajectory.</li> <li><code>device</code>: The device to use for training.</li> <li><code>discretisation_method</code>: How to discretise the overdamped Langevin diffusion. Default 'e-m' for Euler-Maruyama</li> </ul>"},{"location":"api/infer/mcmc/#blackbirds.infer.mcmc.MALA.discretisation_method","title":"<code>discretisation_method = discretisation_method</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/mcmc/#blackbirds.infer.mcmc.MALA.__init__","title":"<code>__init__(*args, discretisation_method: str = 'e-m', **kwargs)</code>","text":""},{"location":"api/infer/mcmc/#blackbirds.infer.mcmc.MALA.initialise_chain","title":"<code>initialise_chain(state, data)</code>","text":""},{"location":"api/infer/mcmc/#blackbirds.infer.mcmc.MALA.step","title":"<code>step(current_state, data, scale: float = 1.0, covariance: torch.Tensor | None = None)</code>","text":"<p>Returns a (torch.Tensor, bool) pair corresponding to (the current state of the chain, whether the current state resulted from an accept or reject decision in the Metropolis step).</p>"},{"location":"api/infer/mcmc/#blackbirds.infer.mcmc.MCMC","title":"<code>blackbirds.infer.mcmc.MCMC</code>","text":"<p>Class that runs an MCMC chain.</p> <p>Arguments</p> <ul> <li><code>kernel</code>: An object with a .step() method that is used to generate the next sample in the chain.</li> <li><code>num_samples</code>: An integer specifying the number of samples to generate in the MCMC chain.</li> <li><code>progress_bar</code>: Whether to display a progress bar during training.</li> <li><code>progress_info</code>: Whether to display loss data during training.</li> </ul>"},{"location":"api/infer/mcmc/#blackbirds.infer.mcmc.MCMC.kernel","title":"<code>kernel = kernel</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/mcmc/#blackbirds.infer.mcmc.MCMC.num_samples","title":"<code>num_samples = num_samples</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/mcmc/#blackbirds.infer.mcmc.MCMC.progress_bar","title":"<code>progress_bar = progress_bar</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/mcmc/#blackbirds.infer.mcmc.MCMC.progress_info","title":"<code>progress_info = progress_info</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/mcmc/#blackbirds.infer.mcmc.MCMC.__init__","title":"<code>__init__(kernel: MCMCKernel, num_samples: int = 100000, progress_bar: bool = True, progress_info: bool = True)</code>","text":""},{"location":"api/infer/mcmc/#blackbirds.infer.mcmc.MCMC.reset","title":"<code>reset()</code>","text":""},{"location":"api/infer/mcmc/#blackbirds.infer.mcmc.MCMC.run","title":"<code>run(initial_state: torch.Tensor, data: torch.Tensor, *args, seed: int = 0, T: int = 1, **kwargs)</code>","text":"<p>Runs the MCMC chain.</p> <p>Arguments</p> <ul> <li><code>initial_state</code>: Starting location of the MCMC chain.</li> <li><code>data</code>: A torch.Tensor containing the data against which the simulator is compared.</li> <li><code>seed</code>: An integer specifying the initial random state of the RNG.</li> <li><code>T</code>: An integer specifying the number of steps between updates of the progress info (if shown).</li> </ul> <p>Additional arguments and keyword arguments can be passed, which will be passed to the kernel  .step() method.</p>"},{"location":"api/infer/smd/","title":"Simulated Minimum Distance","text":""},{"location":"api/infer/smd/#blackbirds.infer.smd.SMD","title":"<code>blackbirds.infer.smd.SMD</code>","text":""},{"location":"api/infer/smd/#blackbirds.infer.smd.SMD.loss_fn","title":"<code>loss_fn = loss</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/smd/#blackbirds.infer.smd.SMD.optimizer","title":"<code>optimizer = optimizer</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/smd/#blackbirds.infer.smd.SMD.gradient_horizon","title":"<code>gradient_horizon = gradient_horizon</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/smd/#blackbirds.infer.smd.SMD.progress_bar","title":"<code>progress_bar = progress_bar</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/smd/#blackbirds.infer.smd.SMD.loss","title":"<code>loss = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/smd/#blackbirds.infer.smd.SMD.__init__","title":"<code>__init__(loss, optimizer, gradient_horizon = None, progress_bar = False)</code>","text":"<p>Simulated Minimum Distance. Finds the point in parameter space that minimizes the distance between the model's output and the observed data given the loss function <code>loss_fn</code>.</p> <p>Arguments:</p> <ul> <li><code>loss</code> : A callable that returns a (differentiable) loss. Needs to take (parameters, data) as input and return a scalar tensor.</li> <li><code>optimizer</code>: A PyTorch optimizer (eg Adam)</li> <li><code>gradient_horizon</code>: The number of steps to look ahead when computing the gradient. If None, defaults to the number of parameters.</li> <li><code>progress_bar</code>: Whether to display a progress bar.</li> </ul>"},{"location":"api/infer/smd/#blackbirds.infer.smd.SMD.run","title":"<code>run(data, n_epochs = 1000, max_epochs_without_improvement = 100, parameters_save_dir = 'best_parameters.pt')</code>","text":"<p>Runs the SMD algorithm for <code>n_epochs</code> epochs.</p> <p>Arguments:</p> <ul> <li><code>data</code>: The observed data.</li> <li><code>n_epochs</code>: The number of epochs to run.</li> <li><code>max_epochs_without_improvement</code>: The number of epochs to run without improvement before stopping.</li> <li><code>parameters_save_dir</code>: The directory to save the best parameters to.</li> </ul>"},{"location":"api/infer/vi/","title":"Variational Inference","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi","title":"<code>blackbirds.infer.vi</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.logger","title":"<code>logger = logging.getLogger('vi')</code>  <code>module-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI","title":"<code>VI</code>","text":"<p>Class to handle (Generalized) Variational Inferece.</p> <p>Arguments:</p> <ul> <li><code>loss</code> : A callable that returns a (differentiable) loss. Needs to take (parameters, data) as input and return a scalar tensor.</li> <li><code>prior</code>: The prior distribution.</li> <li><code>posterior_estimator</code>: The variational distribution that approximates the (generalised) posterior.</li> <li><code>w</code>: The weight of the regularisation loss in the total loss.</li> <li><code>initialize_estimator_to_prior</code>: Whether to fit the posterior estimator to the prior before training.</li> <li><code>initialization_lr</code>: The learning rate to use for the initialization.</li> <li><code>gradient_clipping_norm</code>: The norm to which the gradients are clipped.</li> <li><code>optimizer</code>: The optimizer to use for training.</li> <li><code>n_samples_per_epoch</code>: The number of samples to draw from the variational distribution per epoch.</li> <li><code>n_samples_regularisation</code>: The number of samples used to evaluate the regularisation loss.</li> <li><code>diff_mode</code>: The differentiation mode to use. Can be either 'reverse' or 'forward'.</li> <li><code>gradient_estimation_method</code>: The method to use for estimating the gradients of the loss. Can be either 'pathwise' or 'score'.</li> <li><code>jacobian_chunk_size</code> : The number of rows computed at a time for the model Jacobian. Set to None to compute the full Jacobian at once.</li> <li><code>gradient_horizon</code>: The number of timesteps to use for the gradient horizon. Set 0 to use the full trajectory.</li> <li><code>device</code>: The device to use for training.</li> <li><code>progress_bar</code>: Whether to display a progress bar during training.</li> <li><code>progress_info</code> : Whether to display loss data during training.</li> <li><code>log_tensorboard</code>: Whether to log tensorboard data.</li> <li><code>tensorboard_log_dir</code>: The directory to log tensorboard data to.</li> </ul>"},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.loss","title":"<code>loss = loss</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.prior","title":"<code>prior = prior</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.posterior_estimator","title":"<code>posterior_estimator = posterior_estimator</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.w","title":"<code>w = w</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.initialize_estimator_to_prior","title":"<code>initialize_estimator_to_prior = initialize_estimator_to_prior</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.initialization_lr","title":"<code>initialization_lr = initialization_lr</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.gradient_clipping_norm","title":"<code>gradient_clipping_norm = gradient_clipping_norm</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.optimizer","title":"<code>optimizer = optimizer</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.n_samples_per_epoch","title":"<code>n_samples_per_epoch = n_samples_per_epoch</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.n_samples_regularisation","title":"<code>n_samples_regularisation = n_samples_regularisation</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.progress_bar","title":"<code>progress_bar = progress_bar</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.progress_info","title":"<code>progress_info = progress_info</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.diff_mode","title":"<code>diff_mode = diff_mode</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.gradient_estimation_method","title":"<code>gradient_estimation_method = gradient_estimation_method</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.jacobian_chunk_size","title":"<code>jacobian_chunk_size = jacobian_chunk_size</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.gradient_horizon","title":"<code>gradient_horizon = gradient_horizon</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.device","title":"<code>device = device</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.tensorboard_log_dir","title":"<code>tensorboard_log_dir = tensorboard_log_dir</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.log_tensorboard","title":"<code>log_tensorboard = log_tensorboard</code>  <code>instance-attribute</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.__init__","title":"<code>__init__(loss: Callable, prior: torch.distributions.Distribution, posterior_estimator: torch.nn.Module, w: float = 1.0, initialize_estimator_to_prior: bool = False, initialization_lr: float = 0.001, gradient_clipping_norm: float = np.inf, optimizer: torch.optim.Optimizer | None = None, n_samples_per_epoch: int = 10, n_samples_regularisation: int = 10000, diff_mode: str = 'reverse', gradient_estimation_method: str = 'pathwise', jacobian_chunk_size: int | None = None, gradient_horizon: int | float = np.inf, device: str = 'cpu', progress_bar: bool = True, progress_info: bool = True, log_tensorboard: bool = False, tensorboard_log_dir: str | None = None)</code>","text":""},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.step","title":"<code>step(data)</code>","text":"<p>Performs one training step.</p>"},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.initialize_estimator","title":"<code>initialize_estimator(max_epochs_without_improvement = 50, atol = 0.01)</code>","text":"<p>Initialization step where the estimator is fitted to just the prior.</p>"},{"location":"api/infer/vi/#blackbirds.infer.vi.VI.run","title":"<code>run(data: List[torch.Tensor], n_epochs: int, max_epochs_without_improvement: int = 20)</code>","text":"<p>Runs the calibrator for {n_epochs} epochs. Stops if the loss does not improve for {max_epochs_without_improvement} epochs.</p> <p>Arguments:</p> <ul> <li><code>data</code>: The observed data to calibrate against. It must be given as a list of tensors that matches the output of the model.</li> <li><code>n_epochs</code>: The number of epochs to run the calibrator for.</li> <li><code>max_epochs_without_improvement</code>: The number of epochs without improvement after which the calibrator stops.</li> </ul>"},{"location":"api/infer/vi/#blackbirds.infer.vi.compute_regularisation_loss","title":"<code>compute_regularisation_loss(posterior_estimator: torch.nn.Module, prior: torch.distributions.Distribution, n_samples: int)</code>","text":"<p>Estimates the KL divergence between the posterior and the prior using n_samples through Monte Carlo using</p> \\[ \\mathbb{E}_{q(z|x)}[\\log q(z|x) - \\log p(z)] \\approx \\frac{1}{N} \\sum_{i=1}^N \\left(\\log q(z_i|x) - \\log p(z_i)\\right) \\] <p>Arguments:</p> <ul> <li><code>posterior_estimator</code>: The posterior distribution.</li> <li><code>prior</code>: The prior distribution.</li> <li><code>n_samples</code>: The number of samples to use for the Monte Carlo estimate.</li> </ul> <p>Example</p> <pre><code>    import torch\n    from blackbirds.regularisation import compute_regularisation\n    # define two normal distributions\n    dist1 = torch.distributions.Normal(0, 1)\n    dist2 = torch.distributions.Normal(0, 1)\n    compute_regularisation(dist1, dist2, 1000)\n    # tensor(0.)\n    dist1 = torch.distributions.Normal(0, 1)\n    dist2 = torch.distributions.Normal(1, 1)\n    compute_regularisation(dist1, dist2, 1000)\n    # tensor(0.5)\n</code></pre>"},{"location":"api/infer/vi/#blackbirds.infer.vi.compute_loss_and_jacobian_pathwise","title":"<code>compute_loss_and_jacobian_pathwise(loss_fn: Callable, posterior_estimator: Callable, n_samples: int, observed_outputs: list[torch.Tensor], diff_mode: str = 'reverse', jacobian_chunk_size: int | None = None, gradient_horizon: int = 0, device: str = 'cpu')</code>","text":"<p>Computes the loss and the jacobian of the loss for each sample using a differentiable simulator. That is, we compute</p> \\[ \\eta = \\nabla_\\psi \\mathbb{E}_{p(\\theta | \\psi)} \\left[ \\mathcal{L}(\\theta) \\right], \\] <p>by performing the pathwise gradient (reparameterization trick),</p> \\[ \\eta \\approx \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\psi \\mathcal{L}(\\theta_i(\\psi)). \\] <p>The jacobian is computed using the forward or reverse mode differentiation and the computation is parallelized across the available devices.</p> <p>Arguments:</p> <ul> <li><code>loss_fn</code>: loss function</li> <li><code>posterior_estimator</code>: Object that implements the <code>sample</code> method computing a parameter and its log_prob</li> <li><code>n_samples</code>: number of samples</li> <li><code>observed_outputs</code>: observed outputs</li> <li><code>diff_mode</code>: differentiation mode can be \"reverse\" or \"forward\"</li> <li><code>jacobian_chunk_size</code>: chunk size for the Jacobian computation (set None to get maximum chunk size)</li> <li><code>gradient_horizon</code>: horizon for the gradient computation</li> <li><code>device</code>: device to use for the computation</li> </ul>"},{"location":"api/infer/vi/#blackbirds.infer.vi.compute_and_differentiate_loss_score","title":"<code>compute_and_differentiate_loss_score(loss_fn: Callable, posterior_estimator: torch.nn.Module, n_samples: int, observed_outputs: list[torch.Tensor], device: str = 'cpu')</code>","text":"<p>Computes the loss and the jacobian of the loss for each sample using a differentiable simulator. That is, we compute</p> \\[ \\eta = \\nabla_\\psi \\mathbb{E}_{\\psi \\sim p(\\theta)} \\left[ \\mathcal{L}(\\theta) \\right], \\] <p>by performing the score gradient</p> \\[ \\eta \\approx \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(\\theta_i) \\nabla_\\psi \\log p\\left(\\theta_i | \\psi\\right). \\] <p>The jacobian is computed using the forward or reverse mode differentiation and the computation is parallelized across the available devices.</p> <p>Arguments:</p> <ul> <li><code>loss_fn</code>: loss function</li> <li><code>posterior_estimator</code>: posterior estimator, must implement a sample and a log_prob method</li> <li><code>n_samples</code>: number of samples</li> <li><code>observed_outputs</code>: observed outputs</li> <li><code>device</code>: device to use for the computation</li> </ul>"},{"location":"api/infer/vi/#blackbirds.infer.vi.compute_and_differentiate_loss","title":"<code>compute_and_differentiate_loss(loss_fn: Callable, posterior_estimator: torch.nn.Module, n_samples: int, observed_outputs: list[torch.Tensor], diff_mode: str = 'reverse', gradient_estimation_method: str = 'pathwise', jacobian_chunk_size: int | None = None, gradient_horizon: int = 0, device: str = 'cpu')</code>","text":"<p>Computes and differentiates the loss according to the chosen gradient estimation method and automatic differentiation mechanism. Arguments:</p> <ul> <li><code>loss_fn</code>: loss function</li> <li><code>posterior_estimator</code>: posterior estimator, must implement a sample and a log_prob method</li> <li><code>n_samples</code>: number of samples</li> <li><code>observed_outputs</code>: observed outputs</li> <li><code>diff_mode</code>: differentiation mode can be \"reverse\" or \"forward\"</li> <li><code>gradient_estimation_method</code>: gradient estimation method can be \"pathwise\" or \"score\"</li> <li><code>jacobian_chunk_size</code>: chunk size for the Jacobian computation (set None to get maximum chunk size)</li> <li><code>gradient_horizon</code>: horizon for the gradient computation</li> <li><code>device</code>: device to use for the computation</li> </ul>"},{"location":"api/models/brock_hommes/","title":"Brock &amp; Hommes","text":""},{"location":"api/models/brock_hommes/#blackbirds.models.brock_hommes","title":"<code>blackbirds.models.brock_hommes</code>","text":""},{"location":"api/models/brock_hommes/#blackbirds.models.brock_hommes.BrockHommes","title":"<code>BrockHommes</code>","text":"<p>         Bases: <code>Model</code></p> <p>Differentiable implementation of the Brock and Hommes (1998) model. See equations (39) and (40) of https://arxiv.org/pdf/2202.00625.pdf for reference.</p> <p>Arguments:</p> <ul> <li><code>n_timesteps</code>: Number of timesteps to simulate. Default: 100.</li> </ul>"},{"location":"api/models/brock_hommes/#blackbirds.models.brock_hommes.BrockHommes.n_timesteps","title":"<code>n_timesteps = n_timesteps</code>  <code>instance-attribute</code>","text":""},{"location":"api/models/brock_hommes/#blackbirds.models.brock_hommes.BrockHommes.device","title":"<code>device = device</code>  <code>instance-attribute</code>","text":""},{"location":"api/models/brock_hommes/#blackbirds.models.brock_hommes.BrockHommes.__init__","title":"<code>__init__(n_timesteps = 100, device = 'cpu')</code>","text":""},{"location":"api/models/brock_hommes/#blackbirds.models.brock_hommes.BrockHommes.initialize","title":"<code>initialize(params)</code>","text":""},{"location":"api/models/brock_hommes/#blackbirds.models.brock_hommes.BrockHommes.trim_time_series","title":"<code>trim_time_series(x)</code>","text":""},{"location":"api/models/brock_hommes/#blackbirds.models.brock_hommes.BrockHommes.step","title":"<code>step(params, x)</code>","text":"<p>Runs the model forward for one time-step. Parameters follow the order: log_beta, g1, g2, g3, g4, b1, b2, b3, b4, log_sigma, log_r</p> <p>Arguments:</p> <ul> <li><code>params</code>: A list of parameters. Parameters follow the order: log_beta, g1, g2, g3, g4, b1, b2, b3, b4, log_sigma, log_r</li> <li><code>x</code>: The current state of the model.</li> </ul> <p>Danger</p> <p>beta, sigma, and r are given in log.</p>"},{"location":"api/models/brock_hommes/#blackbirds.models.brock_hommes.BrockHommes.observe","title":"<code>observe(x)</code>","text":""},{"location":"api/models/rama_cont/","title":"Rama Cont","text":""},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont","title":"<code>blackbirds.models.rama_cont</code>","text":""},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont","title":"<code>RamaCont</code>","text":"<p>         Bases: <code>Model</code></p>"},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.n_agents","title":"<code>n_agents = n_agents</code>  <code>instance-attribute</code>","text":""},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.n_timesteps","title":"<code>n_timesteps = n_timesteps</code>  <code>instance-attribute</code>","text":""},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.s","title":"<code>s = s</code>  <code>instance-attribute</code>","text":""},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.sigmoid_k","title":"<code>sigmoid_k = sigmoid_k</code>  <code>instance-attribute</code>","text":""},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.__init__","title":"<code>__init__(n_agents, n_timesteps, s, sigmoid_k)</code>","text":"<p>Implementation of the Rama Cont model from Rama Cont (2005).</p> <p>Arguments:</p> <ul> <li><code>n_agents</code>: Number of agents</li> <li><code>n_timesteps</code>: Number of timesteps</li> <li><code>s</code>: Probability of updating the threshold \\(\\nu_i\\).</li> <li><code>sigmoid_k</code>: Steepness of the sigmoid function.</li> </ul>"},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.initialize","title":"<code>initialize(params)</code>","text":""},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.step","title":"<code>step(params, x)</code>","text":""},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.observe","title":"<code>observe(x)</code>","text":""},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.compute_order_soft","title":"<code>compute_order_soft(epsilon_t, nu_t)</code>","text":""},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.compute_order_hard","title":"<code>compute_order_hard(epsilon_t, nu_t)</code>","text":""},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.compute_order","title":"<code>compute_order(epsilon_t, nu_t)</code>","text":"<p>We do a trick similar to the gumbel-softmax.</p>"},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.compute_returns","title":"<code>compute_returns(order, eta)</code>","text":""},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.compute_new_nu_t","title":"<code>compute_new_nu_t(nu_t, s, returns)</code>","text":""},{"location":"api/models/rama_cont/#blackbirds.models.rama_cont.RamaCont.trim_time_series","title":"<code>trim_time_series(x)</code>","text":""},{"location":"api/models/random_walk/","title":"Random Walk","text":""},{"location":"api/models/random_walk/#blackbirds.models.random_walk","title":"<code>blackbirds.models.random_walk</code>","text":""},{"location":"api/models/random_walk/#blackbirds.models.random_walk.RandomWalk","title":"<code>RandomWalk</code>","text":"<p>         Bases: <code>Model</code></p>"},{"location":"api/models/random_walk/#blackbirds.models.random_walk.RandomWalk.n_timesteps","title":"<code>n_timesteps = n_timesteps</code>  <code>instance-attribute</code>","text":""},{"location":"api/models/random_walk/#blackbirds.models.random_walk.RandomWalk.tau_softmax","title":"<code>tau_softmax = tau_softmax</code>  <code>instance-attribute</code>","text":""},{"location":"api/models/random_walk/#blackbirds.models.random_walk.RandomWalk.__init__","title":"<code>__init__(n_timesteps, tau_softmax = 0.1)</code>","text":"<p>Implements a differentiable random walk.</p> \\[     X_t = \\sum_{i=1}^t (2\\eta - 1), \\] <p>where</p> \\[ \\eta \\sim \\text{Bernoulli}(p). \\] <p>Arguments:</p> <ul> <li><code>n_timesteps</code> (int): Number of timesteps to simulate.</li> <li><code>tau_softmax</code> (float): Temperature parameter for the Gumbel-Softmax</li> </ul>"},{"location":"api/models/random_walk/#blackbirds.models.random_walk.RandomWalk.initialize","title":"<code>initialize(params)</code>","text":""},{"location":"api/models/random_walk/#blackbirds.models.random_walk.RandomWalk.trim_time_series","title":"<code>trim_time_series(x)</code>","text":""},{"location":"api/models/random_walk/#blackbirds.models.random_walk.RandomWalk.step","title":"<code>step(params, x)</code>","text":"<p>Simulates a random walk step using the Gumbel-Softmax trick.</p> <p>Arguments:</p> <ul> <li><code>params</code>: a tensor of shape (1,) containing the logit probability of moving forward at each timestep.</li> <li><code>x</code>: a tensor of shape (n,) containing the time-series of positions.</li> </ul> <p>Danger</p> <p>probability is given in logit, so the input is transformed using the sigmoid function.</p>"},{"location":"api/models/random_walk/#blackbirds.models.random_walk.RandomWalk.observe","title":"<code>observe(x)</code>","text":""},{"location":"api/models/sir/","title":"SIR","text":""},{"location":"api/models/sir/#blackbirds.models.sir","title":"<code>blackbirds.models.sir</code>","text":""},{"location":"api/models/sir/#blackbirds.models.sir.SIR","title":"<code>SIR</code>","text":"<p>         Bases: <code>Model</code></p>"},{"location":"api/models/sir/#blackbirds.models.sir.SIR.n_timesteps","title":"<code>n_timesteps = n_timesteps</code>  <code>instance-attribute</code>","text":""},{"location":"api/models/sir/#blackbirds.models.sir.SIR.graph","title":"<code>graph = torch_geometric.utils.convert.from_networkx(graph).to(device)</code>  <code>instance-attribute</code>","text":""},{"location":"api/models/sir/#blackbirds.models.sir.SIR.mp","title":"<code>mp = SIRMessagePassing(aggr='add', node_dim=-1)</code>  <code>instance-attribute</code>","text":""},{"location":"api/models/sir/#blackbirds.models.sir.SIR.__init__","title":"<code>__init__(graph: networkx.Graph, n_timesteps: int, device: str = 'cpu')</code>","text":"<p>Implements a differentiable SIR model on a graph.</p> <p>Arguments:</p> <ul> <li><code>graph</code>: a networkx graph</li> <li><code>n_timesteps</code>: the number of timesteps to run the model for</li> <li><code>device</code> : device to use (eg. \"cpu\" or \"cuda:0\")</li> </ul>"},{"location":"api/models/sir/#blackbirds.models.sir.SIR.sample_bernoulli_gs","title":"<code>sample_bernoulli_gs(probs: torch.Tensor, tau: float = 0.1)</code>","text":"<p>Samples from a Bernoulli distribution in a diferentiable way using Gumble-Softmax</p> <p>Arguments:</p> <ul> <li>probs: a tensor of shape (n,) containing the probabilities of success for each trial</li> <li>tau: the temperature of the Gumble-Softmax distribution</li> </ul>"},{"location":"api/models/sir/#blackbirds.models.sir.SIR.trim_time_series","title":"<code>trim_time_series(x)</code>","text":""},{"location":"api/models/sir/#blackbirds.models.sir.SIR.initialize","title":"<code>initialize(params: torch.Tensor)</code>","text":"<p>Initializes the model setting the adequate number of initial infections.</p> <p>Arguments:</p> <ul> <li>params: a tensor of shape (3,) containing the log10 of the fraction of infected, beta, and gamma</li> </ul>"},{"location":"api/models/sir/#blackbirds.models.sir.SIR.step","title":"<code>step(params: torch.Tensor, x: torch.Tensor)</code>","text":"<p>Runs the model forward for one timestep.</p> <p>Arguments:</p> <ul> <li>params: a tensor of shape (3,) containing the log10 of the fraction of infected, beta, and gamma</li> <li>x: a tensor of shape (3, n_agents) containing the infected, susceptible, and recovered counts.</li> </ul>"},{"location":"api/models/sir/#blackbirds.models.sir.SIR.observe","title":"<code>observe(x: torch.Tensor)</code>","text":"<p>Returns the total number of infected and recovered agents per time-step</p> <p>Arguments:</p> <ul> <li>x: a tensor of shape (3, n_agents) containing the infected, susceptible, and recovered counts.</li> </ul>"},{"location":"api/models/sir/#blackbirds.models.sir.SIRMessagePassing","title":"<code>SIRMessagePassing</code>","text":"<p>         Bases: <code>torch_geometric.nn.conv.MessagePassing</code></p> <p>Class used to pass messages between agents about their infected status.</p>"},{"location":"api/models/sir/#blackbirds.models.sir.SIRMessagePassing.forward","title":"<code>forward(edge_index: torch.Tensor, infected: torch.Tensor, susceptible: torch.Tensor)</code>","text":"<p>Computes the sum of the product between the node's susceptibility and the neighbors' infected status.</p> <p>Arguments:</p> <ul> <li>edge_index: a tensor of shape (2, n_edges) containing the edge indices</li> <li>infected: a tensor of shape (n_nodes,) containing the infected status of each node</li> <li>susceptible: a tensor of shape (n_nodes,) containing the susceptible status of each node</li> </ul>"},{"location":"api/models/sir/#blackbirds.models.sir.SIRMessagePassing.message","title":"<code>message(x_j, y_i)</code>","text":""},{"location":"examples/mcmc/01-mala-mcmc_conjugate_prior_likelihood_pairs/","title":"MALA MCMC on conjugate prior-likelihood pairs","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as scistats\nimport torch\nimport torch.distributions\n\nfrom blackbirds.infer import mcmc\n</pre> import matplotlib.pyplot as plt import numpy as np import scipy.stats as scistats import torch import torch.distributions  from blackbirds.infer import mcmc In\u00a0[2]: Copied! <pre>mu_0, sigma_0 = 1., 2.\nprior = torch.distributions.normal.Normal(mu_0, sigma_0)\n</pre> mu_0, sigma_0 = 1., 2. prior = torch.distributions.normal.Normal(mu_0, sigma_0) In\u00a0[3]: Copied! <pre>sigma = 1.\n\ndef negative_log_likelihood(theta, data):\n    dist = torch.distributions.normal.Normal(theta, sigma)\n    return - dist.log_prob(data).sum()\n</pre> sigma = 1.  def negative_log_likelihood(theta, data):     dist = torch.distributions.normal.Normal(theta, sigma)     return - dist.log_prob(data).sum() In\u00a0[4]: Copied! <pre>data_size = 3\ndata = torch.distributions.normal.Normal(-1., sigma).sample((data_size,))\n</pre> data_size = 3 data = torch.distributions.normal.Normal(-1., sigma).sample((data_size,)) In\u00a0[5]: Copied! <pre>data\n</pre> data Out[5]: <pre>tensor([-1.5064, -0.6129,  0.4516])</pre> In\u00a0[6]: Copied! <pre>mala = mcmc.MALA(prior, negative_log_likelihood, w=1.)\n</pre> mala = mcmc.MALA(prior, negative_log_likelihood, w=1.) In\u00a0[7]: Copied! <pre>sampler = mcmc.MCMC(mala, 10_000)\n</pre> sampler = mcmc.MCMC(mala, 10_000) In\u00a0[8]: Copied! <pre>trial_samples = sampler.run(torch.tensor([-.5]), data)\n</pre> trial_samples = sampler.run(torch.tensor([-.5]), data) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:17&lt;00:00, 574.66it/s, Acceptance rate=0.288]\n</pre> In\u00a0[9]: Copied! <pre>thinned_trial_samples = torch.stack(trial_samples)[::100].T\nscale = torch.cov(thinned_trial_samples)\n</pre> thinned_trial_samples = torch.stack(trial_samples)[::100].T scale = torch.cov(thinned_trial_samples) In\u00a0[10]: Copied! <pre>mala = mcmc.MALA(prior, negative_log_likelihood, 1.)\nsampler = mcmc.MCMC(mala, 20_000)\npost_samples = sampler.run(torch.tensor([thinned_trial_samples.mean()]), data, scale=scale)\n</pre> mala = mcmc.MALA(prior, negative_log_likelihood, 1.) sampler = mcmc.MCMC(mala, 20_000) post_samples = sampler.run(torch.tensor([thinned_trial_samples.mean()]), data, scale=scale) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20000/20000 [00:35&lt;00:00, 571.00it/s, Acceptance rate=0.759]\n</pre> In\u00a0[11]: Copied! <pre>plt.hist(torch.stack(post_samples).T.numpy()[0, ::100], density=True)\nx = np.linspace(-2, 3., 1000)\nplt.plot(x, scistats.norm.pdf(x, \n                              (mu_0/sigma_0**2 + data.sum()/sigma**2)/(1/sigma_0**2 + data_size/sigma**2),\n                              1/np.sqrt((1/sigma_0**2 + data_size/sigma**2))))\n</pre> plt.hist(torch.stack(post_samples).T.numpy()[0, ::100], density=True) x = np.linspace(-2, 3., 1000) plt.plot(x, scistats.norm.pdf(x,                                (mu_0/sigma_0**2 + data.sum()/sigma**2)/(1/sigma_0**2 + data_size/sigma**2),                               1/np.sqrt((1/sigma_0**2 + data_size/sigma**2)))) Out[11]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f80a8c14640&gt;]</pre> In\u00a0[12]: Copied! <pre>mu_0, sigma_0 = torch.tensor([2., 0.]), torch.tensor([[2., 0.,], [0., 1.]])\nprior = torch.distributions.multivariate_normal.MultivariateNormal(mu_0, sigma_0)\n</pre> mu_0, sigma_0 = torch.tensor([2., 0.]), torch.tensor([[2., 0.,], [0., 1.]]) prior = torch.distributions.multivariate_normal.MultivariateNormal(mu_0, sigma_0) In\u00a0[13]: Copied! <pre>prior_samples = prior.sample((1000,))\nplt.scatter(prior_samples[:,0], prior_samples[:, 1], alpha=0.5)\nplt.ylim([-4,4])\nplt.xlim([-2,6])\n</pre> prior_samples = prior.sample((1000,)) plt.scatter(prior_samples[:,0], prior_samples[:, 1], alpha=0.5) plt.ylim([-4,4]) plt.xlim([-2,6]) Out[13]: <pre>(-2.0, 6.0)</pre> In\u00a0[14]: Copied! <pre>sigma = torch.tensor([[1., 0.4,], [0.4, 2.]])\n\ndef negative_log_likelihood(data, theta):\n    dist = torch.distributions.multivariate_normal.MultivariateNormal(theta, sigma)\n    return - dist.log_prob(data).sum()\n</pre> sigma = torch.tensor([[1., 0.4,], [0.4, 2.]])  def negative_log_likelihood(data, theta):     dist = torch.distributions.multivariate_normal.MultivariateNormal(theta, sigma)     return - dist.log_prob(data).sum() In\u00a0[15]: Copied! <pre>data_size = 3\ntrue_mean = torch.tensor([-1., 2.])\ntrue_density = torch.distributions.multivariate_normal.MultivariateNormal(true_mean, sigma)\ndata = true_density.sample((data_size,))\n</pre> data_size = 3 true_mean = torch.tensor([-1., 2.]) true_density = torch.distributions.multivariate_normal.MultivariateNormal(true_mean, sigma) data = true_density.sample((data_size,)) In\u00a0[16]: Copied! <pre>true_density_samples = true_density.sample((1000,))\nplt.scatter(true_density_samples[:,0], true_density_samples[:,1], alpha=0.5)\n</pre> true_density_samples = true_density.sample((1000,)) plt.scatter(true_density_samples[:,0], true_density_samples[:,1], alpha=0.5) Out[16]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f80a819af50&gt;</pre> In\u00a0[17]: Copied! <pre>data\n</pre> data Out[17]: <pre>tensor([[-1.7573,  0.1746],\n        [-1.2320,  4.3385],\n        [-2.6572,  0.2604]])</pre> In\u00a0[18]: Copied! <pre>mala = mcmc.MALA(prior, negative_log_likelihood, w=1.)\n</pre> mala = mcmc.MALA(prior, negative_log_likelihood, w=1.) In\u00a0[19]: Copied! <pre>sampler = mcmc.MCMC(mala, 20_000)\n</pre> sampler = mcmc.MCMC(mala, 20_000) In\u00a0[20]: Copied! <pre>trial_samples = sampler.run(torch.tensor([-2., -.5]), data)\n</pre> trial_samples = sampler.run(torch.tensor([-2., -.5]), data) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20000/20000 [00:45&lt;00:00, 434.89it/s, Acceptance rate=0.106]\n</pre> In\u00a0[21]: Copied! <pre>thinned_trial_samples = torch.stack(trial_samples)[::100].T\ncov = torch.cov(thinned_trial_samples)\n</pre> thinned_trial_samples = torch.stack(trial_samples)[::100].T cov = torch.cov(thinned_trial_samples) In\u00a0[22]: Copied! <pre>init_state = thinned_trial_samples.mean(dim=1)\n</pre> init_state = thinned_trial_samples.mean(dim=1) In\u00a0[23]: Copied! <pre>mala = mcmc.MALA(prior, negative_log_likelihood, 1.)\nsampler = mcmc.MCMC(mala, 20_000)\npost_samples = sampler.run(init_state, data, covariance=cov)\n</pre> mala = mcmc.MALA(prior, negative_log_likelihood, 1.) sampler = mcmc.MCMC(mala, 20_000) post_samples = sampler.run(init_state, data, covariance=cov) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20000/20000 [00:44&lt;00:00, 450.00it/s, Acceptance rate=0.642]\n</pre> In\u00a0[24]: Copied! <pre>inv_sigma_0 = torch.inverse(sigma_0)\ninv_sigma = torch.inverse(sigma)\ninv_additions = torch.inverse(inv_sigma_0 + data_size*inv_sigma)\ntrue_mean = torch.matmul(inv_additions, \n                         (torch.matmul(inv_sigma_0, mu_0) + data_size*torch.matmul(inv_sigma, data.mean(dim=0))))\ntrue_cov = torch.inverse(inv_sigma_0 + data_size*inv_sigma)\ntrue_post = torch.distributions.multivariate_normal.MultivariateNormal(true_mean, true_cov)\ntrue_post_samples = true_post.sample((1000,))\n</pre> inv_sigma_0 = torch.inverse(sigma_0) inv_sigma = torch.inverse(sigma) inv_additions = torch.inverse(inv_sigma_0 + data_size*inv_sigma) true_mean = torch.matmul(inv_additions,                           (torch.matmul(inv_sigma_0, mu_0) + data_size*torch.matmul(inv_sigma, data.mean(dim=0)))) true_cov = torch.inverse(inv_sigma_0 + data_size*inv_sigma) true_post = torch.distributions.multivariate_normal.MultivariateNormal(true_mean, true_cov) true_post_samples = true_post.sample((1000,)) In\u00a0[25]: Copied! <pre>post_samples_numpy = torch.stack(post_samples).T.numpy()\nplt.scatter(post_samples_numpy[0, ::100], post_samples_numpy[1, ::100], alpha=0.5, c='b')\nplt.scatter(true_post_samples[:, 0], true_post_samples[:, 1], alpha=0.5, c='r', marker='x')\n</pre> post_samples_numpy = torch.stack(post_samples).T.numpy() plt.scatter(post_samples_numpy[0, ::100], post_samples_numpy[1, ::100], alpha=0.5, c='b') plt.scatter(true_post_samples[:, 0], true_post_samples[:, 1], alpha=0.5, c='r', marker='x') Out[25]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f80a81e5930&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/mcmc/01-mala-mcmc_conjugate_prior_likelihood_pairs/#mala-mcmc-on-conjugate-prior-likelihood-pairs","title":"MALA MCMC on conjugate prior-likelihood pairs\u00b6","text":""},{"location":"examples/mcmc/01-mala-mcmc_conjugate_prior_likelihood_pairs/#conjugate-normal","title":"Conjugate Normal\u00b6","text":""},{"location":"examples/mcmc/01-mala-mcmc_conjugate_prior_likelihood_pairs/#sampling","title":"Sampling\u00b6","text":""},{"location":"examples/mcmc/01-mala-mcmc_conjugate_prior_likelihood_pairs/#conjugate-multivariate-normal","title":"Conjugate multivariate Normal\u00b6","text":""},{"location":"examples/mcmc/01-mala-mcmc_conjugate_prior_likelihood_pairs/#sampling","title":"Sampling\u00b6","text":""},{"location":"examples/smd/01-random_walk/","title":"Simulated Minimum Distance","text":"<p>In this notebook we illustrate the interface to do point-wise estimates of parameters.</p> <p>We will use the RandomWalk model as an example. In this model, at each step, the agent can take a forward or a backward step. The probability to do the former is given by <code>p</code>.</p> In\u00a0[1]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\n\nfrom blackbirds.models.random_walk import RandomWalk\n</pre> import torch import matplotlib.pyplot as plt  from blackbirds.models.random_walk import RandomWalk In\u00a0[2]: Copied! <pre>rw = RandomWalk(n_timesteps=100)\n# the random walk takes as input the logit of the parameter\np = 0.25\nlogit_p = torch.logit(torch.tensor([p]))\ntrue_data = rw.run_and_observe(logit_p)\n\nplt.plot(true_data[0])\n</pre> rw = RandomWalk(n_timesteps=100) # the random walk takes as input the logit of the parameter p = 0.25 logit_p = torch.logit(torch.tensor([p])) true_data = rw.run_and_observe(logit_p)  plt.plot(true_data[0]) Out[2]: <pre>[&lt;matplotlib.lines.Line2D at 0x14acb7340&gt;]</pre> <p>We now try to recover the true parameter by minimizing the L2 distance between the two time series:</p> In\u00a0[3]: Copied! <pre>from blackbirds.smd import SMD\n</pre> from blackbirds.smd import SMD In\u00a0[4]: Copied! <pre>initial_parameter = torch.logit(torch.tensor([0.5]))\ninitial_parameter.requires_grad = True\noptimizer = torch.optim.Adam([initial_parameter], lr=1e-2)\n\nsmd = SMD(rw, loss_fn = torch.nn.MSELoss(), optimizer=optimizer, progress_bar=True)\nsmd.run(true_data, n_epochs=1000, max_epochs_without_improvement=100)\n</pre> initial_parameter = torch.logit(torch.tensor([0.5])) initial_parameter.requires_grad = True optimizer = torch.optim.Adam([initial_parameter], lr=1e-2)  smd = SMD(rw, loss_fn = torch.nn.MSELoss(), optimizer=optimizer, progress_bar=True) smd.run(true_data, n_epochs=1000, max_epochs_without_improvement=100) <pre> 18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                                      | 179/1000 [00:01&lt;00:07, 105.00it/s, loss=40.4, best loss=4.71, epochs since improv.=100]\n</pre> In\u00a0[5]: Copied! <pre># load best parameters\nbest_parameters = torch.load(\"best_parameters.pt\")\nprint(f\"Best parameters are {torch.sigmoid(best_parameters)}\")\n</pre> # load best parameters best_parameters = torch.load(\"best_parameters.pt\") print(f\"Best parameters are {torch.sigmoid(best_parameters)}\") <pre>Best parameters are tensor([0.3626], grad_fn=&lt;SigmoidBackward0&gt;)\n</pre> In\u00a0[8]: Copied! <pre>fitted_data = rw.run_and_observe(best_parameters)\n</pre> fitted_data = rw.run_and_observe(best_parameters) In\u00a0[9]: Copied! <pre>f, ax = plt.subplots()\nax.plot(true_data[0], color = \"black\", label = \"data\")\nax.plot(fitted_data[0].detach().cpu().numpy(), color = \"C0\", label = \"fit\")\nax.legend()\n</pre> f, ax = plt.subplots() ax.plot(true_data[0], color = \"black\", label = \"data\") ax.plot(fitted_data[0].detach().cpu().numpy(), color = \"C0\", label = \"fit\") ax.legend() Out[9]: <pre>&lt;matplotlib.legend.Legend at 0x14e5a1180&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/smd/01-random_walk/#simulated-minimum-distance","title":"Simulated Minimum Distance\u00b6","text":""},{"location":"examples/variational_inference/01-random_walk/","title":"Random Walk","text":"<p>Let's suppose we have a random walk process defined by $$ x(t+1) = x(t) + 2 \\varepsilon - 1, \\; \\; \\; \\; \\varepsilon \\sim \\mathrm{Bernoulli}(p) $$ where $p$ is the probability of doing a step forward.</p> <p>In this notebook, we create some synthetic data for a value of $p$ and we try to recover it through inference.</p> In\u00a0[1]: Copied! <pre>from blackbirds.models.random_walk import RandomWalk\nfrom blackbirds.infer.vi import VI\nfrom blackbirds.posterior_estimators import TrainableGaussian\nfrom blackbirds.simulate import simulate_and_observe_model\n\nimport torch\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> from blackbirds.models.random_walk import RandomWalk from blackbirds.infer.vi import VI from blackbirds.posterior_estimators import TrainableGaussian from blackbirds.simulate import simulate_and_observe_model  import torch import matplotlib.pyplot as plt import pandas as pd In\u00a0[2]: Copied! <pre>rw = RandomWalk(n_timesteps=100)\n</pre> rw = RandomWalk(n_timesteps=100) In\u00a0[3]: Copied! <pre>true_p = torch.logit(torch.tensor(0.25))\ntrue_data = rw.observe(rw.run(torch.tensor([true_p])))\n\nplt.plot(true_data[0].numpy())\n</pre> true_p = torch.logit(torch.tensor(0.25)) true_data = rw.observe(rw.run(torch.tensor([true_p])))  plt.plot(true_data[0].numpy()) Out[3]: <pre>[&lt;matplotlib.lines.Line2D at 0x7fc524a7c8b0&gt;]</pre> <p>The loss callable needs to take as input the model parameters and true data and return the loss value.</p> In\u00a0[4]: Copied! <pre>class L2Loss:\n    def __init__(self, model):\n        self.model = model\n        self.loss_fn = torch.nn.MSELoss()\n    def __call__(self, params, data):\n        observed_outputs = simulate_and_observe_model(self.model, params)\n        return self.loss_fn(observed_outputs[0], data[0])\n</pre> class L2Loss:     def __init__(self, model):         self.model = model         self.loss_fn = torch.nn.MSELoss()     def __call__(self, params, data):         observed_outputs = simulate_and_observe_model(self.model, params)         return self.loss_fn(observed_outputs[0], data[0]) In\u00a0[5]: Copied! <pre>posterior_estimator = TrainableGaussian([0.], 1.0)\nprior = torch.distributions.Normal(true_p + 0.2, 1)\noptimizer = torch.optim.Adam(posterior_estimator.parameters(), 1e-2)\nloss = L2Loss(rw)\n\nvi = VI(loss, posterior_estimator=posterior_estimator, prior=prior, data=true_data, optimizer=optimizer, w = 0)\n</pre> posterior_estimator = TrainableGaussian([0.], 1.0) prior = torch.distributions.Normal(true_p + 0.2, 1) optimizer = torch.optim.Adam(posterior_estimator.parameters(), 1e-2) loss = L2Loss(rw)  vi = VI(loss, posterior_estimator=posterior_estimator, prior=prior, data=true_data, optimizer=optimizer, w = 0) In\u00a0[6]: Copied! <pre># we can now train the estimator for a 100 epochs\nvi.run(1000, max_epochs_without_improvement=100)\n</pre> # we can now train the estimator for a 100 epochs vi.run(1000, max_epochs_without_improvement=100) <pre> 34%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                     | 337/1000 [02:03&lt;04:03,  2.73it/s, loss=56, reg.=0, total=56, best loss=20.7, epochs since improv.=100]\n</pre> <p>The model stops when it hits a certain amount of epochs without improvement. The run function returns the loss per epoch as well as the best model weights. Let's have a look at the loss first:</p> In\u00a0[8]: Copied! <pre>df = pd.DataFrame(vi.losses_hist)\ndf.head()\n</pre> df = pd.DataFrame(vi.losses_hist) df.head() Out[8]: total loss regularisation 0 1253.936768 1253.936768 0.0 1 1521.354492 1521.354492 0.0 2 1223.053467 1223.053467 0.0 3 1124.776245 1124.776245 0.0 4 795.603943 795.603943 0.0 In\u00a0[9]: Copied! <pre>df.plot(y=\"total\", logy=True)\n</pre> df.plot(y=\"total\", logy=True) Out[9]: <pre>&lt;Axes: &gt;</pre> In\u00a0[13]: Copied! <pre># We can now load the best model\nposterior_estimator.load_state_dict(vi.best_estimator_state_dict)\n</pre> # We can now load the best model posterior_estimator.load_state_dict(vi.best_estimator_state_dict) Out[13]: <pre>&lt;All keys matched successfully&gt;</pre> In\u00a0[14]: Copied! <pre># and plot the posterior\nwith torch.no_grad():\n    samples = posterior_estimator.sample(20000)[0].flatten().cpu()\nplt.hist(samples, density=True, bins=100);\nplt.axvline(true_p, label = \"true value\", color = \"black\", linestyle=\":\")\n</pre> # and plot the posterior with torch.no_grad():     samples = posterior_estimator.sample(20000)[0].flatten().cpu() plt.hist(samples, density=True, bins=100); plt.axvline(true_p, label = \"true value\", color = \"black\", linestyle=\":\") Out[14]: <pre>&lt;matplotlib.lines.Line2D at 0x7fc518c49330&gt;</pre> In\u00a0[15]: Copied! <pre># compare the predictions to the synthetic data:\n\nf, ax = plt.subplots()\n\nfor i in range(50):\n    with torch.no_grad():\n        sim_rw = rw.observe(rw.run(posterior_estimator.sample(1)[0]))[0].numpy()\n    ax.plot(sim_rw, color = \"C0\", alpha=0.5)\n    \nax.plot([], [], color = \"C0\", label = \"predicted\")\nax.plot(true_data[0], color = \"black\", linewidth=2, label = \"data\")\n\nax.legend()\n</pre> # compare the predictions to the synthetic data:  f, ax = plt.subplots()  for i in range(50):     with torch.no_grad():         sim_rw = rw.observe(rw.run(posterior_estimator.sample(1)[0]))[0].numpy()     ax.plot(sim_rw, color = \"C0\", alpha=0.5)      ax.plot([], [], color = \"C0\", label = \"predicted\") ax.plot(true_data[0], color = \"black\", linewidth=2, label = \"data\")  ax.legend() Out[15]: <pre>&lt;matplotlib.legend.Legend at 0x7fc513eece50&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/variational_inference/01-random_walk/#random-walk","title":"Random Walk\u00b6","text":""},{"location":"examples/variational_inference/01-random_walk/#generating-synthetic-data","title":"Generating synthetic data\u00b6","text":""},{"location":"examples/variational_inference/01-random_walk/#defining-the-loss","title":"Defining the loss\u00b6","text":""},{"location":"examples/variational_inference/02-SIR/","title":"SIR model","text":"<p>Here we calibrate a differentiable version of the SIR model. We use the exact same model as  https://ndlib.readthedocs.io/en/latest/reference/models/epidemics/SIR.html, but implemented in a differentiable way.</p> In\u00a0[1]: Copied! <pre>from blackbirds.models.sir import SIR\nfrom blackbirds.infer.vi import VI\nfrom blackbirds.simulate import simulate_and_observe_model\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport networkx\nimport normflows as nf\nimport pygtc\n</pre> from blackbirds.models.sir import SIR from blackbirds.infer.vi import VI from blackbirds.simulate import simulate_and_observe_model  import torch import numpy as np import matplotlib.pyplot as plt import pandas as pd import networkx import normflows as nf import pygtc In\u00a0[2]: Copied! <pre>device = \"cpu\"\n</pre> device = \"cpu\" In\u00a0[3]: Copied! <pre># generate a random graph\nn_agents = 1000\ngraph = networkx.watts_strogatz_graph(n_agents, 10, 0.1)\n</pre> # generate a random graph n_agents = 1000 graph = networkx.watts_strogatz_graph(n_agents, 10, 0.1) In\u00a0[4]: Copied! <pre>sir = SIR(graph, n_timesteps=100, device=device)\n</pre> sir = SIR(graph, n_timesteps=100, device=device) In\u00a0[5]: Copied! <pre>%%time\n# the simulator takes as parameters the log10 of the fraction of initial cases, beta, and gamma parameters\ntrue_parameters = torch.log10(torch.tensor([0.05, 0.05, 0.05], device=device))\ndata = sir.run_and_observe(true_parameters)\ntrue_infected, true_recovered = data\n</pre> %%time # the simulator takes as parameters the log10 of the fraction of initial cases, beta, and gamma parameters true_parameters = torch.log10(torch.tensor([0.05, 0.05, 0.05], device=device)) data = sir.run_and_observe(true_parameters) true_infected, true_recovered = data <pre>CPU times: user 43.5 ms, sys: 19.3 ms, total: 62.8 ms\nWall time: 48 ms\n</pre> In\u00a0[6]: Copied! <pre>f, ax = plt.subplots()\nax.plot(true_infected.cpu(), label = \"active infected\")\nax.set_xlabel(\"Time-step\")\nax.plot(true_recovered.cpu(), label = \"cumulative recovered\")\nax.legend()\n</pre> f, ax = plt.subplots() ax.plot(true_infected.cpu(), label = \"active infected\") ax.set_xlabel(\"Time-step\") ax.plot(true_recovered.cpu(), label = \"cumulative recovered\") ax.legend() Out[6]: <pre>&lt;matplotlib.legend.Legend at 0x2a951e380&gt;</pre> <p>We construct the flow using the normflows library (https://github.com/VincentStimper/normalizing-flows )</p> <p>In this case we define Neural Spline Flow with 4 transformations, each parametrised by 2 layers with 64 hidden units.</p> In\u00a0[7]: Copied! <pre>def make_flow(n_parameters, device):\n    K = 16\n    torch.manual_seed(0)\n    flows = []\n    for i in range(K):\n        flows.append(nf.flows.MaskedAffineAutoregressive(n_parameters, 20, num_blocks=2))\n        flows.append(nf.flows.Permute(n_parameters, mode=\"swap\"))\n    q0 = nf.distributions.DiagGaussian(n_parameters)\n    nfm = nf.NormalizingFlow(q0=q0, flows=flows)\n    return nfm.to(device)\n</pre> def make_flow(n_parameters, device):     K = 16     torch.manual_seed(0)     flows = []     for i in range(K):         flows.append(nf.flows.MaskedAffineAutoregressive(n_parameters, 20, num_blocks=2))         flows.append(nf.flows.Permute(n_parameters, mode=\"swap\"))     q0 = nf.distributions.DiagGaussian(n_parameters)     nfm = nf.NormalizingFlow(q0=q0, flows=flows)     return nfm.to(device) In\u00a0[8]: Copied! <pre># Plot the inital flow:\nflow = make_flow(len(true_parameters), device=device)\nsamples = flow.sample(10000)[0].cpu().detach().numpy()\n\npygtc.plotGTC([samples], truths=true_parameters.cpu().numpy(), figureSize=7, paramNames=[r\"$I_0$\", r\"$\\beta$\", r\"$\\gamma$\"]);\n</pre> # Plot the inital flow: flow = make_flow(len(true_parameters), device=device) samples = flow.sample(10000)[0].cpu().detach().numpy()  pygtc.plotGTC([samples], truths=true_parameters.cpu().numpy(), figureSize=7, paramNames=[r\"$I_0$\", r\"$\\beta$\", r\"$\\gamma$\"]); <p>Let's also plot runs sampled from the untrained flow, to compare later with the trained flow.</p> In\u00a0[9]: Copied! <pre>f, ax = plt.subplots()\n\nfor i in range(15):\n    with torch.no_grad():\n        sim_sir = sir.run_and_observe(flow.sample(1)[0][0])\n    ax.plot(sim_sir[0].cpu().numpy(), color = \"C0\", alpha=0.5)\n    ax.plot(sim_sir[1].cpu().numpy(), color = \"C1\", alpha=0.5)\n    \nax.plot([], [], color = \"C0\", label = \"predicted infected\")\nax.plot([], [], color = \"C1\", label = \"predicted recovered\")\nax.plot(data[0].cpu(), color = \"black\", linewidth=2, label = \"data infected\")\nax.plot(data[1].cpu(), color = \"black\", linewidth=2, label = \"data recovered\", linestyle=\"--\")\nax.legend()\n</pre> f, ax = plt.subplots()  for i in range(15):     with torch.no_grad():         sim_sir = sir.run_and_observe(flow.sample(1)[0][0])     ax.plot(sim_sir[0].cpu().numpy(), color = \"C0\", alpha=0.5)     ax.plot(sim_sir[1].cpu().numpy(), color = \"C1\", alpha=0.5)      ax.plot([], [], color = \"C0\", label = \"predicted infected\") ax.plot([], [], color = \"C1\", label = \"predicted recovered\") ax.plot(data[0].cpu(), color = \"black\", linewidth=2, label = \"data infected\") ax.plot(data[1].cpu(), color = \"black\", linewidth=2, label = \"data recovered\", linestyle=\"--\") ax.legend() Out[9]: <pre>&lt;matplotlib.legend.Legend at 0x2aa81a590&gt;</pre> In\u00a0[10]: Copied! <pre>torch.manual_seed(0)\n\nclass L2Loss:\n    def __init__(self, model):\n        self.model = model\n        self.loss_fn = torch.nn.MSELoss()\n        \n    def __call__(self, params, data):\n        observed_outputs = simulate_and_observe_model(self.model, params, gradient_horizon=0)\n        return self.loss_fn(observed_outputs[0], data[0])\n\nprior = torch.distributions.MultivariateNormal(-2.0 * torch.ones(3, device=device), torch.eye(3, device=device))\nloss = L2Loss(sir)\noptimizer = torch.optim.AdamW(flow.parameters(), lr=1e-3)\nw = 100\n\nvi = VI(loss = loss,\n        posterior_estimator = flow,\n        prior=prior,\n        data=data,\n        optimizer=optimizer,\n        w=w,\n        n_samples_per_epoch=10,\n        log_tensorboard=True,\n        device=device\n       )\n\n# and we run for 1000 epochs, stopping if the loss doesn't improve in 100 epochs.\nvi.run(n_epochs=1000, max_epochs_without_improvement=50);\n</pre> torch.manual_seed(0)  class L2Loss:     def __init__(self, model):         self.model = model         self.loss_fn = torch.nn.MSELoss()              def __call__(self, params, data):         observed_outputs = simulate_and_observe_model(self.model, params, gradient_horizon=0)         return self.loss_fn(observed_outputs[0], data[0])  prior = torch.distributions.MultivariateNormal(-2.0 * torch.ones(3, device=device), torch.eye(3, device=device)) loss = L2Loss(sir) optimizer = torch.optim.AdamW(flow.parameters(), lr=1e-3) w = 100  vi = VI(loss = loss,         posterior_estimator = flow,         prior=prior,         data=data,         optimizer=optimizer,         w=w,         n_samples_per_epoch=10,         log_tensorboard=True,         device=device        )  # and we run for 1000 epochs, stopping if the loss doesn't improve in 100 epochs. vi.run(n_epochs=1000, max_epochs_without_improvement=50); <pre> 22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                      | 223/1000 [07:05&lt;24:44,  1.91s/it, loss=6.05e+3, reg.=561, total=6.61e+3, best loss=2.73e+3, epochs since improv.=50]\n</pre> In\u00a0[11]: Copied! <pre># Let's have a look at the loss over epochs:\n\ndf = pd.DataFrame(vi.losses_hist)\ndf.plot(logy=True)\n</pre> # Let's have a look at the loss over epochs:  df = pd.DataFrame(vi.losses_hist) df.plot(logy=True) Out[11]: <pre>&lt;Axes: &gt;</pre> In\u00a0[12]: Copied! <pre># We can load the best model to check the results\nflow.load_state_dict(vi.best_estimator_state_dict)\n</pre> # We can load the best model to check the results flow.load_state_dict(vi.best_estimator_state_dict) Out[12]: <pre>&lt;All keys matched successfully&gt;</pre> In\u00a0[13]: Copied! <pre># Plot the final flow posterior approximator and compare it to the real parameters:\nsamples = flow.sample(50000)[0].cpu().detach().numpy()\n\n#corner(samples, truths=true_parameters.numpy(), smooth=2, range = ((-2, -1.0), (-1.7, -1.0), (-1.4, -1.25)), labels=[\"initial_fraction_infected\", r\"$\\beta$\", r\"$\\gamma$\"]);\n#corner(samples, truths=true_parameters.numpy(), smooth=2, labels=[\"initial_fraction_infected\", r\"$\\beta$\", r\"$\\gamma$\"]);\npygtc.plotGTC([samples], truths=true_parameters.cpu().numpy(), figureSize=10, priors=[(-2, 1) for i in range(3)], paramRanges=[(-3.0, -0.5) for i in range(3)]);\n</pre> # Plot the final flow posterior approximator and compare it to the real parameters: samples = flow.sample(50000)[0].cpu().detach().numpy()  #corner(samples, truths=true_parameters.numpy(), smooth=2, range = ((-2, -1.0), (-1.7, -1.0), (-1.4, -1.25)), labels=[\"initial_fraction_infected\", r\"$\\beta$\", r\"$\\gamma$\"]); #corner(samples, truths=true_parameters.numpy(), smooth=2, labels=[\"initial_fraction_infected\", r\"$\\beta$\", r\"$\\gamma$\"]); pygtc.plotGTC([samples], truths=true_parameters.cpu().numpy(), figureSize=10, priors=[(-2, 1) for i in range(3)], paramRanges=[(-3.0, -0.5) for i in range(3)]); In\u00a0[14]: Copied! <pre># compare the predictions to the synthetic data:\n\nf, ax = plt.subplots()\n\nfor i in range(25):\n    with torch.no_grad():\n        sim_sir = sir.observe(sir.run((flow.sample(1)[0][0])))\n    ax.plot(sim_sir[0].cpu().numpy(), color = \"C0\", alpha=0.5)\n    ax.plot(sim_sir[1].cpu().numpy(), color = \"C1\", alpha=0.5)\n    \nax.plot([], [], color = \"C0\", label = \"predicted infected\")\nax.plot([], [], color = \"C1\", label = \"predicted recovered\")\nax.plot(data[0].cpu(), color = \"black\", linewidth=2, label = \"data infected\")\nax.plot(data[1].cpu(), color = \"black\", linewidth=2, label = \"data recovered\", linestyle=\"--\")\n\nax.legend()\n</pre> # compare the predictions to the synthetic data:  f, ax = plt.subplots()  for i in range(25):     with torch.no_grad():         sim_sir = sir.observe(sir.run((flow.sample(1)[0][0])))     ax.plot(sim_sir[0].cpu().numpy(), color = \"C0\", alpha=0.5)     ax.plot(sim_sir[1].cpu().numpy(), color = \"C1\", alpha=0.5)      ax.plot([], [], color = \"C0\", label = \"predicted infected\") ax.plot([], [], color = \"C1\", label = \"predicted recovered\") ax.plot(data[0].cpu(), color = \"black\", linewidth=2, label = \"data infected\") ax.plot(data[1].cpu(), color = \"black\", linewidth=2, label = \"data recovered\", linestyle=\"--\")  ax.legend() Out[14]: <pre>&lt;matplotlib.legend.Legend at 0x2aaf95930&gt;</pre>"},{"location":"examples/variational_inference/02-SIR/#sir-model","title":"SIR model\u00b6","text":""},{"location":"examples/variational_inference/02-SIR/#generating-synthetic-true-data","title":"Generating synthetic true data\u00b6","text":""},{"location":"examples/variational_inference/02-SIR/#approximating-the-posterior-by-a-normalizing-flow","title":"Approximating the posterior by a normalizing flow\u00b6","text":""},{"location":"examples/variational_inference/02-SIR/#train-the-flow","title":"Train the flow\u00b6","text":""},{"location":"examples/variational_inference/03-brock_hommes/","title":"Brock and Hommes model","text":"In\u00a0[1]: Copied! <pre>from blackbirds.models.brock_hommes import BrockHommes\nfrom blackbirds.infer import VI\nfrom blackbirds.utils import soft_minimum, soft_maximum\nfrom blackbirds.simulate import simulate_and_observe_model\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport networkx\nimport normflows as nf\nimport pygtc\n</pre> from blackbirds.models.brock_hommes import BrockHommes from blackbirds.infer import VI from blackbirds.utils import soft_minimum, soft_maximum from blackbirds.simulate import simulate_and_observe_model  import torch import numpy as np import matplotlib.pyplot as plt import pandas as pd import networkx import normflows as nf import pygtc In\u00a0[2]: Copied! <pre>class MMDLoss:\n    def __init__(self, model, data):\n        self.model = model\n        self.y = data[0]\n        self.y_matrix = self.y.reshape(1,-1,1)\n        self.y_sigma = torch.median(torch.pow(torch.cdist(self.y_matrix, self.y_matrix), 2))\n        ny = self.y.shape[0]\n        self.kyy = (torch.exp( - torch.pow(torch.cdist(self.y_matrix, self.y_matrix), 2) / self.y_sigma ) - torch.eye(ny)).sum() / (ny * (ny - 1))\n        \n    def __call__(self, params, data):\n        x = simulate_and_observe_model(self.model, params, gradient_horizon=0)[0]\n        nx = x.shape[0]\n        x_matrix = x.reshape(1,-1,1)\n        kxx = torch.exp( - torch.pow(torch.cdist(x_matrix, x_matrix), 2) / self.y_sigma )\n        #kxx = torch.nan_to_num(kxx, 0.)\n        kxx = (kxx - torch.eye(nx)).sum() / (nx * (nx - 1))\n        kxy = torch.exp( - torch.pow(torch.cdist(x_matrix, self.y_matrix), 2) / self.y_sigma )\n        #kxy = torch.nan_to_num(kxy, 0.)\n        kxy = kxy.mean()\n        return kxx + self.kyy - 2 * kxy\n</pre> class MMDLoss:     def __init__(self, model, data):         self.model = model         self.y = data[0]         self.y_matrix = self.y.reshape(1,-1,1)         self.y_sigma = torch.median(torch.pow(torch.cdist(self.y_matrix, self.y_matrix), 2))         ny = self.y.shape[0]         self.kyy = (torch.exp( - torch.pow(torch.cdist(self.y_matrix, self.y_matrix), 2) / self.y_sigma ) - torch.eye(ny)).sum() / (ny * (ny - 1))              def __call__(self, params, data):         x = simulate_and_observe_model(self.model, params, gradient_horizon=0)[0]         nx = x.shape[0]         x_matrix = x.reshape(1,-1,1)         kxx = torch.exp( - torch.pow(torch.cdist(x_matrix, x_matrix), 2) / self.y_sigma )         #kxx = torch.nan_to_num(kxx, 0.)         kxx = (kxx - torch.eye(nx)).sum() / (nx * (nx - 1))         kxy = torch.exp( - torch.pow(torch.cdist(x_matrix, self.y_matrix), 2) / self.y_sigma )         #kxy = torch.nan_to_num(kxy, 0.)         kxy = kxy.mean()         return kxx + self.kyy - 2 * kxy <p>Following Dyer et al we fix</p> In\u00a0[3]: Copied! <pre>H = 4\nlog_r = np.log(1.0)\nlog_sigma = np.log(0.04)\ng1, b1, b4, = 0, 0, 0\ng4 = 1.01\nlog_beta = np.log(120)\nn_timesteps = 100\n\nclass CustomBrockHommes(BrockHommes):\n    def step(self, params, x):\n        expanded_params = torch.tensor([log_beta, g1, 0, 0, g4, b1, 0, 0, b4, log_sigma, log_r])\n        expanded_params[2] = params[0] # g2\n        expanded_params[3] = params[1] # g3\n        expanded_params[6] = params[2] # b2\n        expanded_params[7] = params[3] # b2\n        return super().step(expanded_params, x)\n</pre> H = 4 log_r = np.log(1.0) log_sigma = np.log(0.04) g1, b1, b4, = 0, 0, 0 g4 = 1.01 log_beta = np.log(120) n_timesteps = 100  class CustomBrockHommes(BrockHommes):     def step(self, params, x):         expanded_params = torch.tensor([log_beta, g1, 0, 0, g4, b1, 0, 0, b4, log_sigma, log_r])         expanded_params[2] = params[0] # g2         expanded_params[3] = params[1] # g3         expanded_params[6] = params[2] # b2         expanded_params[7] = params[3] # b2         return super().step(expanded_params, x) In\u00a0[4]: Copied! <pre>model = CustomBrockHommes(n_timesteps)\n</pre> model = CustomBrockHommes(n_timesteps) In\u00a0[5]: Copied! <pre>g2 = 0.9\ng3 = 0.9\nb2 = 0.2\nb3 = -0.2\n\ntrue_parameters = torch.tensor([g2, g3, b2, b3])\ntorch.manual_seed(0)\nx = model.run(true_parameters)\ndata = model.observe(x)\n</pre> g2 = 0.9 g3 = 0.9 b2 = 0.2 b3 = -0.2  true_parameters = torch.tensor([g2, g3, b2, b3]) torch.manual_seed(0) x = model.run(true_parameters) data = model.observe(x) In\u00a0[6]: Copied! <pre>plt.plot(data[0].cpu())\n</pre> plt.plot(data[0].cpu()) Out[6]: <pre>[&lt;matplotlib.lines.Line2D at 0x15f687b20&gt;]</pre> In\u00a0[7]: Copied! <pre>def make_flow():\n    torch.manual_seed(0)\n    base = nf.distributions.base.DiagGaussian(len(true_parameters))\n    num_layers = 5\n    latent_size = len(true_parameters)\n    flows = []\n    for i in range(num_layers):\n        param_map = nf.nets.MLP([2, 50, 50, latent_size], init_zeros=True)\n        flows.append(nf.flows.AffineCouplingBlock(param_map))\n        flows.append(nf.flows.Permute(latent_size, mode='swap'))\n    return nf.NormalizingFlow(base, flows)\n</pre> def make_flow():     torch.manual_seed(0)     base = nf.distributions.base.DiagGaussian(len(true_parameters))     num_layers = 5     latent_size = len(true_parameters)     flows = []     for i in range(num_layers):         param_map = nf.nets.MLP([2, 50, 50, latent_size], init_zeros=True)         flows.append(nf.flows.AffineCouplingBlock(param_map))         flows.append(nf.flows.Permute(latent_size, mode='swap'))     return nf.NormalizingFlow(base, flows) In\u00a0[8]: Copied! <pre>torch.manual_seed(0)\nprior = torch.distributions.MultivariateNormal(torch.tensor([0.5, 0.5, 0.5, -0.5]), 1.0 * torch.eye(len(true_parameters)))\nestimator = make_flow()\nloss = MMDLoss(model, data)\noptimizer = torch.optim.AdamW(estimator.parameters(), lr=1e-3)\nvi = VI(loss = loss, \n        posterior_estimator = estimator, \n        prior=prior, \n        data=data, \n        optimizer=optimizer, \n        n_samples_per_epoch=10,\n        w=0.001,\n        log_tensorboard=True,\n        gradient_estimation_method=\"pathwise\",\n        gradient_clipping_norm=1.0,\n        gradient_horizon=0\n    )\n\nvi.run(n_epochs=1000, max_epochs_without_improvement=50);\n</pre> torch.manual_seed(0) prior = torch.distributions.MultivariateNormal(torch.tensor([0.5, 0.5, 0.5, -0.5]), 1.0 * torch.eye(len(true_parameters))) estimator = make_flow() loss = MMDLoss(model, data) optimizer = torch.optim.AdamW(estimator.parameters(), lr=1e-3) vi = VI(loss = loss,          posterior_estimator = estimator,          prior=prior,          data=data,          optimizer=optimizer,          n_samples_per_epoch=10,         w=0.001,         log_tensorboard=True,         gradient_estimation_method=\"pathwise\",         gradient_clipping_norm=1.0,         gradient_horizon=0     )  vi.run(n_epochs=1000, max_epochs_without_improvement=50); <pre> 35%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                           | 351/1000 [04:24&lt;08:08,  1.33it/s, loss=-.00783, reg.=0.0097, total=0.00187, best loss=-.00936, epochs since improv.=50]\n</pre> In\u00a0[9]: Copied! <pre># We can load the best model to check the results\nestimator.load_state_dict(vi.best_estimator_state_dict)\n</pre> # We can load the best model to check the results estimator.load_state_dict(vi.best_estimator_state_dict) Out[9]: <pre>&lt;All keys matched successfully&gt;</pre> In\u00a0[10]: Copied! <pre># Plot the final flow posterior approximator and compare it to the real parameters:\nsamples = estimator.sample(10000)[0].detach().numpy()\nsamples_prior = prior.sample((10000,)).numpy()\n\npygtc.plotGTC([samples, samples_prior], figureSize=10, truths = true_parameters.numpy(), chainLabels = [\"flow\", \"prior\"]);\n#corner(samples, truths=true_parameters.numpy(), smooth=2);#, range=[(0,1) for i in range(3)]+ [(-1,0)]);\n</pre> # Plot the final flow posterior approximator and compare it to the real parameters: samples = estimator.sample(10000)[0].detach().numpy() samples_prior = prior.sample((10000,)).numpy()  pygtc.plotGTC([samples, samples_prior], figureSize=10, truths = true_parameters.numpy(), chainLabels = [\"flow\", \"prior\"]); #corner(samples, truths=true_parameters.numpy(), smooth=2);#, range=[(0,1) for i in range(3)]+ [(-1,0)]); In\u00a0[11]: Copied! <pre>n_samples = 1\nwith torch.no_grad():\n    flow_samples = estimator.sample((n_samples))[0]\n\nf, ax = plt.subplots(figsize=(20, 3))\nfor i in range(n_samples):\n    prediction = model.observe(model.run(flow_samples[i]))\n    ax.plot(prediction[0].cpu(), color = \"C0\", label = \"predicted\")\n    true = model.observe(model.run(true_parameters))\n    ax.plot(true[0], color = \"black\", label = \"true\")\n\nax.legend()\n</pre> n_samples = 1 with torch.no_grad():     flow_samples = estimator.sample((n_samples))[0]  f, ax = plt.subplots(figsize=(20, 3)) for i in range(n_samples):     prediction = model.observe(model.run(flow_samples[i]))     ax.plot(prediction[0].cpu(), color = \"C0\", label = \"predicted\")     true = model.observe(model.run(true_parameters))     ax.plot(true[0], color = \"black\", label = \"true\")  ax.legend() Out[11]: <pre>&lt;matplotlib.legend.Legend at 0x297e5eef0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/variational_inference/03-brock_hommes/#brock-and-hommes-model","title":"Brock and Hommes model\u00b6","text":""},{"location":"examples/variational_inference/04-score_vs_pathwise_gradient/","title":"Score vs Pathwise Gradient","text":"In\u00a0[1]: Copied! <pre>from blackbirds.models.sir import SIR\nfrom blackbirds.infer import VI\nfrom blackbirds.simulate import simulate_and_observe_model\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport networkx\nimport pygtc\nimport normflows as nf\n</pre> from blackbirds.models.sir import SIR from blackbirds.infer import VI from blackbirds.simulate import simulate_and_observe_model  import torch import numpy as np import matplotlib.pyplot as plt import pandas as pd import networkx import pygtc import normflows as nf In\u00a0[2]: Copied! <pre>n_agents = 1000\ngraph = networkx.watts_strogatz_graph(n_agents, 10, 0.1)\nsir = SIR(graph, n_timesteps=100)\n</pre> n_agents = 1000 graph = networkx.watts_strogatz_graph(n_agents, 10, 0.1) sir = SIR(graph, n_timesteps=100) In\u00a0[3]: Copied! <pre>true_parameters = torch.tensor([0.05, 0.05, 0.05]).log10()\ndata = sir.observe(sir.run(true_parameters))\n</pre> true_parameters = torch.tensor([0.05, 0.05, 0.05]).log10() data = sir.observe(sir.run(true_parameters)) In\u00a0[4]: Copied! <pre>prior = torch.distributions.MultivariateNormal(-2.0 * torch.ones(3), torch.eye(3))\n</pre> prior = torch.distributions.MultivariateNormal(-2.0 * torch.ones(3), torch.eye(3)) In\u00a0[5]: Copied! <pre>def setup_flow():\n    K = 4\n    torch.manual_seed(0)\n    \n    latent_size = 3\n    hidden_units = 64\n    hidden_layers = 2\n    \n    flows = []\n    for i in range(K):\n        flows += [nf.flows.AutoregressiveRationalQuadraticSpline(latent_size, hidden_layers, hidden_units)]\n        flows += [nf.flows.LULinearPermute(latent_size)]\n    \n    # Set prior and q0\n    q0 = nf.distributions.DiagGaussian(3, trainable=False)\n        \n    # Construct flow model\n    flow = nf.NormalizingFlow(q0=q0, flows=flows)\n    return flow\n\nclass L2Loss:\n    def __init__(self, model):\n        self.model = model\n        self.loss_fn = torch.nn.MSELoss()\n    def __call__(self, params, data):\n        observed_outputs = simulate_and_observe_model(self.model, params, gradient_horizon=0)\n        return self.loss_fn(observed_outputs[0], data[0])\n</pre> def setup_flow():     K = 4     torch.manual_seed(0)          latent_size = 3     hidden_units = 64     hidden_layers = 2          flows = []     for i in range(K):         flows += [nf.flows.AutoregressiveRationalQuadraticSpline(latent_size, hidden_layers, hidden_units)]         flows += [nf.flows.LULinearPermute(latent_size)]          # Set prior and q0     q0 = nf.distributions.DiagGaussian(3, trainable=False)              # Construct flow model     flow = nf.NormalizingFlow(q0=q0, flows=flows)     return flow  class L2Loss:     def __init__(self, model):         self.model = model         self.loss_fn = torch.nn.MSELoss()     def __call__(self, params, data):         observed_outputs = simulate_and_observe_model(self.model, params, gradient_horizon=0)         return self.loss_fn(observed_outputs[0], data[0]) In\u00a0[6]: Copied! <pre>def train_estimator(gradient_estimation_mode, n_samples_per_epoch):\n    torch.manual_seed(0)\n    posterior_estimator = setup_flow() \n    optimizer = torch.optim.Adam(posterior_estimator.parameters(), 1e-3)\n    loss = L2Loss(sir)\n    vi = VI(loss = loss, \n            posterior_estimator=posterior_estimator, \n            prior=prior, \n            data=data, \n            optimizer=optimizer,\n            w = 10.0, \n            n_samples_per_epoch=n_samples_per_epoch,\n            gradient_estimation_method=gradient_estimation_mode,\n   )\n    vi.run(250, max_epochs_without_improvement=250)\n    return vi\n</pre> def train_estimator(gradient_estimation_mode, n_samples_per_epoch):     torch.manual_seed(0)     posterior_estimator = setup_flow()      optimizer = torch.optim.Adam(posterior_estimator.parameters(), 1e-3)     loss = L2Loss(sir)     vi = VI(loss = loss,              posterior_estimator=posterior_estimator,              prior=prior,              data=data,              optimizer=optimizer,             w = 10.0,              n_samples_per_epoch=n_samples_per_epoch,             gradient_estimation_method=gradient_estimation_mode,    )     vi.run(250, max_epochs_without_improvement=250)     return vi In\u00a0[7]: Copied! <pre>%%time\nvi_pathwise = train_estimator(\"pathwise\", 5)\n</pre> %%time vi_pathwise = train_estimator(\"pathwise\", 5) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 250/250 [05:52&lt;00:00,  1.41s/it, loss=1.47e+3, reg.=99.6, total=1.57e+3, best loss=491, epochs since improv.=55]</pre> <pre>CPU times: user 5min 39s, sys: 2min 26s, total: 8min 5s\nWall time: 5min 52s\n</pre> <pre>\n</pre> In\u00a0[8]: Copied! <pre>%%time\nvi_score = train_estimator(\"score\", 5)\n</pre> %%time vi_score = train_estimator(\"score\", 5) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 250/250 [03:07&lt;00:00,  1.33it/s, loss=8.17e+4, reg.=187, total=8.19e+4, best loss=4.33e+4, epochs since improv.=111]</pre> <pre>CPU times: user 2min 49s, sys: 2min 23s, total: 5min 13s\nWall time: 3min 7s\n</pre> <pre>\n</pre> In\u00a0[9]: Copied! <pre>f, ax = plt.subplots()\nax.plot(vi_pathwise.losses_hist[\"total\"], label = \"pathwise\")\nax.plot(vi_score.losses_hist[\"total\"], label = \"score function\")\nax.set_yscale(\"log\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epoch\")\nax.legend(title=\"Gradient method\")\n</pre> f, ax = plt.subplots() ax.plot(vi_pathwise.losses_hist[\"total\"], label = \"pathwise\") ax.plot(vi_score.losses_hist[\"total\"], label = \"score function\") ax.set_yscale(\"log\") ax.set_ylabel(\"Loss\") ax.set_xlabel(\"Epoch\") ax.legend(title=\"Gradient method\") Out[9]: <pre>&lt;matplotlib.legend.Legend at 0x2ab4e3250&gt;</pre> In\u00a0[10]: Copied! <pre>vi_pathwise.posterior_estimator.load_state_dict(vi_pathwise.best_estimator_state_dict)\nvi_score.posterior_estimator.load_state_dict(vi_score.best_estimator_state_dict)\n</pre> vi_pathwise.posterior_estimator.load_state_dict(vi_pathwise.best_estimator_state_dict) vi_score.posterior_estimator.load_state_dict(vi_score.best_estimator_state_dict) Out[10]: <pre>&lt;All keys matched successfully&gt;</pre> In\u00a0[11]: Copied! <pre>samples_pw = vi_pathwise.posterior_estimator.sample(10000)[0].detach().numpy()\nsamples_score = vi_score.posterior_estimator.sample(10000)[0].detach().numpy()\n\npygtc.plotGTC(chains=[samples_pw, samples_score],\n              figureSize=8, \n              truths = true_parameters.numpy(), \n              chainLabels=[\"pathwise\", \"score function\"], \n              paramNames=[\"inital_fraction\", r\"$\\beta$\", r\"$\\gamma$\"]);\n</pre> samples_pw = vi_pathwise.posterior_estimator.sample(10000)[0].detach().numpy() samples_score = vi_score.posterior_estimator.sample(10000)[0].detach().numpy()  pygtc.plotGTC(chains=[samples_pw, samples_score],               figureSize=8,                truths = true_parameters.numpy(),                chainLabels=[\"pathwise\", \"score function\"],                paramNames=[\"inital_fraction\", r\"$\\beta$\", r\"$\\gamma$\"]); In\u00a0[12]: Copied! <pre># compare the predictions to the synthetic data:\n\nf, ax = plt.subplots(1, 2, figsize=(10,4), sharex=True, sharey=True)\nalpha=0.7\n\nfor i in range(25):\n    with torch.no_grad():\n        sim_sir_pw = sir.run_and_observe(vi_pathwise.posterior_estimator.sample(1)[0][0])\n        ax[0].plot(sim_sir_pw[0].numpy(), color = \"C0\", alpha=alpha)\n        ax[0].plot(sim_sir_pw[1].numpy(), color = \"C1\", alpha=alpha)\n        sim_sir_score = sir.run_and_observe(vi_score.posterior_estimator.sample(1)[0][0])\n        ax[1].plot(sim_sir_score[0].numpy(), color = \"C0\", alpha=alpha)\n        ax[1].plot(sim_sir_score[1].numpy(), color = \"C1\", alpha=alpha)\n    \nax[1].plot([], [], color = \"C0\", label = \"predicted infected\")\nax[1].plot([], [], color = \"C1\", label = \"predicted recovered\")\nfor i in range(2):\n    ax[i].plot(data[0], color = \"black\", linewidth=2, label = \"data infected\")\n    ax[i].plot(data[1], color = \"black\", linewidth=2, label = \"data recovered\", linestyle=\"--\")\n    ax[i].set_xlabel(\"Time-step\")\n\nax[0].set_title(\"Pathwise gradient estimation\")\nax[1].set_title(\"Score function gradient estimation\")\n\nax[1].legend(loc=\"center left\", bbox_to_anchor=(1,0.5))\nplt.subplots_adjust(wspace=0.05, hspace=0.05)\n</pre> # compare the predictions to the synthetic data:  f, ax = plt.subplots(1, 2, figsize=(10,4), sharex=True, sharey=True) alpha=0.7  for i in range(25):     with torch.no_grad():         sim_sir_pw = sir.run_and_observe(vi_pathwise.posterior_estimator.sample(1)[0][0])         ax[0].plot(sim_sir_pw[0].numpy(), color = \"C0\", alpha=alpha)         ax[0].plot(sim_sir_pw[1].numpy(), color = \"C1\", alpha=alpha)         sim_sir_score = sir.run_and_observe(vi_score.posterior_estimator.sample(1)[0][0])         ax[1].plot(sim_sir_score[0].numpy(), color = \"C0\", alpha=alpha)         ax[1].plot(sim_sir_score[1].numpy(), color = \"C1\", alpha=alpha)      ax[1].plot([], [], color = \"C0\", label = \"predicted infected\") ax[1].plot([], [], color = \"C1\", label = \"predicted recovered\") for i in range(2):     ax[i].plot(data[0], color = \"black\", linewidth=2, label = \"data infected\")     ax[i].plot(data[1], color = \"black\", linewidth=2, label = \"data recovered\", linestyle=\"--\")     ax[i].set_xlabel(\"Time-step\")  ax[0].set_title(\"Pathwise gradient estimation\") ax[1].set_title(\"Score function gradient estimation\")  ax[1].legend(loc=\"center left\", bbox_to_anchor=(1,0.5)) plt.subplots_adjust(wspace=0.05, hspace=0.05) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/variational_inference/04-score_vs_pathwise_gradient/#score-vs-pathwise-gradient","title":"Score vs Pathwise Gradient\u00b6","text":""},{"location":"examples/variational_inference/05-gpu_parallelisation/","title":"05 gpu parallelisation","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nThis scripts shows how to run blackbirds in parallel using MPI4PY.\nThe parallelization is done across the number of parameters that are sampled\nin each epoch from the posterior candidate.\n\nAs an example we consider the SIR model.\n\"\"\"\nimport argparse\nimport torch\nimport networkx\nimport normflows as nf\nimport numpy as np\n</pre> \"\"\" This scripts shows how to run blackbirds in parallel using MPI4PY. The parallelization is done across the number of parameters that are sampled in each epoch from the posterior candidate.  As an example we consider the SIR model. \"\"\" import argparse import torch import networkx import normflows as nf import numpy as np In\u00a0[\u00a0]: Copied! <pre>from blackbirds.models.sir import SIR\nfrom blackbirds.calibrator import Calibrator\nfrom blackbirds.mpi_setup import mpi_rank\n</pre> from blackbirds.models.sir import SIR from blackbirds.calibrator import Calibrator from blackbirds.mpi_setup import mpi_rank In\u00a0[\u00a0]: Copied! <pre>def make_model(n_agents, n_timesteps):\n    graph = networkx.watts_strogatz_graph(n_agents, 10, 0.1)\n    return SIR(graph=graph, n_timesteps=n_timesteps)\n</pre> def make_model(n_agents, n_timesteps):     graph = networkx.watts_strogatz_graph(n_agents, 10, 0.1)     return SIR(graph=graph, n_timesteps=n_timesteps) In\u00a0[\u00a0]: Copied! <pre>def make_flow(device):\n    # Define flows\n    torch.manual_seed(0)\n    K = 4\n    latent_size = 3\n    hidden_units = 64\n    hidden_layers = 2\n\n    flows = []\n    for _ in range(K):\n        flows += [\n            nf.flows.AutoregressiveRationalQuadraticSpline(\n                latent_size, hidden_layers, hidden_units\n            )\n        ]\n        flows += [nf.flows.LULinearPermute(latent_size)]\n\n    # Set prior and q0\n    q0 = nf.distributions.DiagGaussian(3, trainable=False)\n\n    # Construct flow model\n    flow = nf.NormalizingFlow(q0=q0, flows=flows)\n    return flow.to(device)\n</pre> def make_flow(device):     # Define flows     torch.manual_seed(0)     K = 4     latent_size = 3     hidden_units = 64     hidden_layers = 2      flows = []     for _ in range(K):         flows += [             nf.flows.AutoregressiveRationalQuadraticSpline(                 latent_size, hidden_layers, hidden_units             )         ]         flows += [nf.flows.LULinearPermute(latent_size)]      # Set prior and q0     q0 = nf.distributions.DiagGaussian(3, trainable=False)      # Construct flow model     flow = nf.NormalizingFlow(q0=q0, flows=flows)     return flow.to(device) In\u00a0[\u00a0]: Copied! <pre>def train_flow(flow, model, true_data, n_epochs, n_samples_per_epoch, device):\n    torch.manual_seed(0)\n    # Define a prior\n    prior = torch.distributions.MultivariateNormal(\n        -2.0 * torch.ones(3, device=device), torch.eye(3, device=device)\n    )\n\n    optimizer = torch.optim.AdamW(flow.parameters(), lr=1e-3)\n\n    # We set the regularisation weight to 10.\n    w = 100\n\n    # Note that we can track the progress of the training by using tensorboard.\n    # tensorboard --logdir=runs\n    calibrator = Calibrator(\n        model=model,\n        posterior_estimator=flow,\n        prior=prior,\n        data=true_data,\n        optimizer=optimizer,\n        w=w,\n        n_samples_per_epoch=n_samples_per_epoch,\n        device=device,\n    )\n\n    # and we run for 500 epochs without early stopping.\n    calibrator.run(n_epochs=n_epochs, max_epochs_without_improvement=np.inf)\n</pre> def train_flow(flow, model, true_data, n_epochs, n_samples_per_epoch, device):     torch.manual_seed(0)     # Define a prior     prior = torch.distributions.MultivariateNormal(         -2.0 * torch.ones(3, device=device), torch.eye(3, device=device)     )      optimizer = torch.optim.AdamW(flow.parameters(), lr=1e-3)      # We set the regularisation weight to 10.     w = 100      # Note that we can track the progress of the training by using tensorboard.     # tensorboard --logdir=runs     calibrator = Calibrator(         model=model,         posterior_estimator=flow,         prior=prior,         data=true_data,         optimizer=optimizer,         w=w,         n_samples_per_epoch=n_samples_per_epoch,         device=device,     )      # and we run for 500 epochs without early stopping.     calibrator.run(n_epochs=n_epochs, max_epochs_without_improvement=np.inf) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    # parse arguments from cli\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--n_epochs\", type=int, default=500)\n    parser.add_argument(\"--n_agents\", type=int, default=1000)\n    parser.add_argument(\"--n_timesteps\", type=int, default=100)\n    parser.add_argument(\"--n_samples_per_epoch\", type=int, default=5)\n    parser.add_argument(\"--device_ids\", default=[\"cpu\"], nargs=\"+\")\n    args = parser.parse_args()\n\n    # device of this rank\n    device = args.device_ids[mpi_rank]\n\n    model = make_model(args.n_agents, args.n_timesteps)\n    true_parameters = torch.tensor(\n        [0.05, 0.05, 0.05], device=device\n    ).log10()  # SIR takes log parameters\n    true_data = model(true_parameters)\n    flow = make_flow(device)\n    train_flow(\n        flow, model, true_data, args.n_epochs, args.n_samples_per_epoch, device=device\n    )\n</pre> if __name__ == \"__main__\":     # parse arguments from cli     parser = argparse.ArgumentParser()     parser.add_argument(\"--n_epochs\", type=int, default=500)     parser.add_argument(\"--n_agents\", type=int, default=1000)     parser.add_argument(\"--n_timesteps\", type=int, default=100)     parser.add_argument(\"--n_samples_per_epoch\", type=int, default=5)     parser.add_argument(\"--device_ids\", default=[\"cpu\"], nargs=\"+\")     args = parser.parse_args()      # device of this rank     device = args.device_ids[mpi_rank]      model = make_model(args.n_agents, args.n_timesteps)     true_parameters = torch.tensor(         [0.05, 0.05, 0.05], device=device     ).log10()  # SIR takes log parameters     true_data = model(true_parameters)     flow = make_flow(device)     train_flow(         flow, model, true_data, args.n_epochs, args.n_samples_per_epoch, device=device     )"},{"location":"examples/variational_inference/06-classical_posterior/","title":"Variational Inference with classical posterior","text":"<p>In this notebook we set the VI loss to be the negative log-likelihood, to recover the classical posterior.</p> In\u00a0[2]: Copied! <pre>from blackbirds.models.random_walk import RandomWalk\nfrom blackbirds.infer.vi import VI\nfrom blackbirds.posterior_estimators import TrainableGaussian\nfrom blackbirds.simulate import simulate_and_observe_model\n\nimport torch\nimport math\nimport matplotlib.pyplot as plt\nimport pandas as pd\n</pre> from blackbirds.models.random_walk import RandomWalk from blackbirds.infer.vi import VI from blackbirds.posterior_estimators import TrainableGaussian from blackbirds.simulate import simulate_and_observe_model  import torch import math import matplotlib.pyplot as plt import pandas as pd In\u00a0[9]: Copied! <pre>rw = RandomWalk(n_timesteps=40)\n</pre> rw = RandomWalk(n_timesteps=40) In\u00a0[10]: Copied! <pre>true_p = torch.logit(torch.tensor(0.25))\ndata = rw.run_and_observe(torch.tensor([true_p]))\n\nplt.plot(data[0].numpy())\n</pre> true_p = torch.logit(torch.tensor(0.25)) data = rw.run_and_observe(torch.tensor([true_p]))  plt.plot(data[0].numpy()) Out[10]: <pre>[&lt;matplotlib.lines.Line2D at 0x14eb08a00&gt;]</pre> In\u00a0[24]: Copied! <pre>class LogLikelihoodLoss:\n    def __init__(self, model):\n        self.model = model\n        \n    def __call__(self, params, data):\n        N = self.model.n_timesteps\n        p = torch.sigmoid(params[0])\n        lp = 0\n        for n in range(1, N+1):\n            if data[0][n] == data[0][n-1] + 1:\n                lp += p.log()\n            else:\n                lp += (1 - p).log()\n            #k = int(data[0][n].item())\n            #likelihood = math.comb(n, (n+k)//2) * p**((n+k)//2) * (1-p)**((n-k)//2)\n            #lp += likelihood.log()\n        return -lp\n</pre> class LogLikelihoodLoss:     def __init__(self, model):         self.model = model              def __call__(self, params, data):         N = self.model.n_timesteps         p = torch.sigmoid(params[0])         lp = 0         for n in range(1, N+1):             if data[0][n] == data[0][n-1] + 1:                 lp += p.log()             else:                 lp += (1 - p).log()             #k = int(data[0][n].item())             #likelihood = math.comb(n, (n+k)//2) * p**((n+k)//2) * (1-p)**((n-k)//2)             #lp += likelihood.log()         return -lp In\u00a0[48]: Copied! <pre>posterior_estimator = TrainableGaussian([0.], 1.0)\nprior = torch.distributions.Normal(true_p + 0.1, 0.1)\noptimizer = torch.optim.Adam(posterior_estimator.parameters(), 1e-2)\nll = LogLikelihoodLoss(rw)\n\nvi = VI(ll, posterior_estimator=posterior_estimator, prior=prior, data=data, optimizer=optimizer, w = 1.0, n_samples_regularisation=1000)\n</pre> posterior_estimator = TrainableGaussian([0.], 1.0) prior = torch.distributions.Normal(true_p + 0.1, 0.1) optimizer = torch.optim.Adam(posterior_estimator.parameters(), 1e-2) ll = LogLikelihoodLoss(rw)  vi = VI(ll, posterior_estimator=posterior_estimator, prior=prior, data=data, optimizer=optimizer, w = 1.0, n_samples_regularisation=1000) In\u00a0[49]: Copied! <pre># we can now train the estimator for a 100 epochs\nvi.run(1000, max_epochs_without_improvement=100)\n</pre> # we can now train the estimator for a 100 epochs vi.run(1000, max_epochs_without_improvement=100) <pre> 28%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                              | 284/1000 [00:05&lt;00:13, 51.15it/s, loss=23.5, reg.=0.692, total=24.2, best loss=23.5, epochs since improv.=100]\n</pre> In\u00a0[50]: Copied! <pre>df = pd.DataFrame(vi.losses_hist)\ndf.plot()\n</pre> df = pd.DataFrame(vi.losses_hist) df.plot() Out[50]: <pre>&lt;Axes: &gt;</pre> In\u00a0[51]: Copied! <pre># We can now load the best model\nposterior_estimator.load_state_dict(vi.best_estimator_state_dict)\n</pre> # We can now load the best model posterior_estimator.load_state_dict(vi.best_estimator_state_dict) Out[51]: <pre>&lt;All keys matched successfully&gt;</pre> In\u00a0[52]: Copied! <pre>data_diff = data[0].diff()\np_hat = 0.5 * (1 + 1 / (len(data[0])-1) * data_diff.sum())\np_hat\n</pre> data_diff = data[0].diff() p_hat = 0.5 * (1 + 1 / (len(data[0])-1) * data_diff.sum()) p_hat                 Out[52]: <pre>tensor(0.5000)</pre> In\u00a0[53]: Copied! <pre># and plot the posterior\nwith torch.no_grad():\n    samples = posterior_estimator.sample(20000)[0].flatten().cpu()\nplt.hist(torch.sigmoid(samples), density=True, bins=100);\nplt.axvline(torch.sigmoid(true_p), label = \"true value\", color = \"black\", linestyle=\":\")\n</pre> # and plot the posterior with torch.no_grad():     samples = posterior_estimator.sample(20000)[0].flatten().cpu() plt.hist(torch.sigmoid(samples), density=True, bins=100); plt.axvline(torch.sigmoid(true_p), label = \"true value\", color = \"black\", linestyle=\":\") Out[53]: <pre>&lt;matplotlib.lines.Line2D at 0x14fafa3e0&gt;</pre> In\u00a0[54]: Copied! <pre># compare the predictions to the synthetic data:\n\nf, ax = plt.subplots()\n\nfor i in range(50):\n    with torch.no_grad():\n        sim_rw = rw.run_and_observe(posterior_estimator.sample(1)[0])[0].numpy()\n    ax.plot(sim_rw, color = \"C0\", alpha=0.5)\n    \nax.plot([], [], color = \"C0\", label = \"predicted\")\nax.plot(data[0], color = \"black\", linewidth=2, label = \"data\")\n\nax.legend()\n</pre> # compare the predictions to the synthetic data:  f, ax = plt.subplots()  for i in range(50):     with torch.no_grad():         sim_rw = rw.run_and_observe(posterior_estimator.sample(1)[0])[0].numpy()     ax.plot(sim_rw, color = \"C0\", alpha=0.5)      ax.plot([], [], color = \"C0\", label = \"predicted\") ax.plot(data[0], color = \"black\", linewidth=2, label = \"data\")  ax.legend() Out[54]: <pre>&lt;matplotlib.legend.Legend at 0x14ecafbe0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/variational_inference/06-classical_posterior/#variational-inference-with-classical-posterior","title":"Variational Inference with classical posterior\u00b6","text":""}]}